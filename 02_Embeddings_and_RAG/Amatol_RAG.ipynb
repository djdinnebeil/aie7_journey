{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - ðŸš§ Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import UniversalFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
        "\n",
        "> NOTE: We're using blogs from PMarca (Marc Andreessen) as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'UniversalFileLoader' object has no attribute 'load_documents'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text_loader = UniversalFileLoader(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m documents = \u001b[43mtext_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_documents\u001b[49m()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mlen\u001b[39m(documents)\n",
            "\u001b[31mAttributeError\u001b[39m: 'UniversalFileLoader' object has no attribute 'load_documents'"
          ]
        }
      ],
      "source": [
        "text_loader = UniversalFileLoader(\"data\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Suggestions to My Readers\\n\\nT have written this book to share with you, my readers, that which\\n1 know and teach. It incorporates portions of my little blue book,\\nHeal Your Body, which has become widely accepted as an authorita-\\ntive work on the mental patterns that create dis-eases in the body.\\n\\n1 have had hundreds of letters from readers asking me to share\\nmore of my information. Many persons who have worked with me\\nas private clients, and those who have taken my workshops here and\\nabroad, have requested I take the time to write this book.\\n\\nI have set up this book to take you through a session, just as I\\nwould if you came to me as a private client and attended one of my\\nworkshops.\\n\\nIf you will do the exercises progressively as they appear in the\\nbook, by the time you have finished, you will have begun to change\\nyour life.\\n\\nI suggest you read through the book once. Then slowly read it\\nagain, only this time do each exercise in depth. Give yourself time to\\nwork with each one.\\n\\nIf you can, work through the exercises with a friend or with a\\nmember of your family.\\n\\nEach chapter opens with an affirmation. Each of these is good to\\nuse when you are working on that area of your life. Take two or three\\ndays to study and work with each chapter. Keep saying and writing\\nthe affirmation that opens the chapter.\\n\\n \\n\\x0cYOU CAN HEAL YOUR LIFE\\n\\nThe chapters close with a treatment. This is a flow of positive\\nideas designed to change consciousness. Read over this treatment\\nseveral times a day.\\n\\nI close this book by sharing with you my own story. I know it will\\nshow you that no matter where we have come from or how lowly it\\nwas, we can totally change our lives for the better.\\n\\nKnow that when you work with these ideas, my loving support is\\n\\nwith you.\\n\\n \\n\\x0cSome Points\\nof My Philosophy\\nWe are each responsible for all of our experiences.\\nEvery thought we think is creating our future.\\nThe point of power is always in the present moment.\\n\\nEveryone suffers from self-hatred and guilt.\\n\\nThe bottom line for everyone is,\\nâ€œTm not good enough.â€\\n\\nIts only a thought, and a thought can be changed.\\nWe create every so-called illness in our body.\\n\\nResentment, criticism, and guilt\\nare the most damaging patterns.\\n\\nReleasing resentment will dissolve even cancer.\\nWe must release the past and forgive everyone.\\nWe must be willing to begin to learn to love ourselves.\\n\\nSelf-approval and self-acceptance in the now\\nare the keys to positive changes.\\n\\nWhen we really love ourselves, everything in our life works.\\n\\n \\n\\x0c\\\\\\n\\nIn the infinity of life where I am, all is perfect,\\nwhole, and complete, and yet life is ever changing.\\nThere is no beginning and no end,\\nonfy a constant cycling and recycling\\nof substance and experiences.\\n\\nLife is never stuck or static or stale,\\nfor each moment is ever new and fresh.\\n\\nTam one with the very Power that created me, and this Power\\nhas given me the power to create my own circumstances.\\nI rejoice in the knowledge that I have the power\\nof my own mind to use in any way I choose.\\nEvery moment of life is a new beginning point\\nas we move from the old. This moment is a new point\\nof beginning for me right here and right now.\\n\\nAll is well in my world.\\n\\n \\n\\x0c', '1All Aboard for Amatol, New Jersey\\nDaniel J. Dinnebeil\\nAs a result of Americaâ€™s entry into World War \\nI, on April 6, 1917, Atlantic County received a great expansion of its industrial economic \\nbase. Among the largest result of this expansion was the construction of Amatol, a planned shell-loading muni-tions plant and workersâ€™ village that once occupied a tract of 6,000 acres between East Hammonton and Elwood. Amatol and other World War I inspired construction eï¬€orts, such as Camp Dix in Burlington County, are fas-cinating, in part due to the rapidity of their construction. In just nine monthsâ€™ time, the new town and manufac-tory at Amatol were completed and reached a population of 7,000. Today, the Pine Barrens have all but reclaimed the former site of Amatol.H/i.sc/s.sc/t.sc/o.sc/r.sc/y.sc\\n/T_he history of Amatol began in December 1917 \\nwith the incorporation of the Atlantic Loading Com-pany.\\n1 Acting under the aegis of the United States \\nGovernment, the Atlantic Loading Company received governmental contracts to construct a large munitions plant and an accompanying town to house workers. In early 1918, the Atlantic Loading Company had initially planned to build the munitions plant and town near Camp Dix, but the war department had concerns over water pollution and the safety of the campâ€™s personnel, and told the contractor to locate elsewhere.\\n2 In testimony \\ngiven regarding Amatol, Lieutenant Colonel R. H. Haw-kins stated that Camp Dix was not selected â€œbecause \\nTHE RESPONSE TO THE CALL FOR SPEED\\nTo the left, a work crew newly arrived in the South Jersey Pine Barrens, March 4, 1918, prepares to commence construction of Amatol, the World War I shell-loading facility and workersâ€™ town near Hammonton, New Jersey. The manufacturing plant was /f_irst built and loading operations began in July 1918. To the right is completed workersâ€™ town built in use nine months later. From Victor F. Hammel, Construction and Operation of a Shell Loading Plant and the Town of Amatol, New Jersey, 4.2SoJourn\\nthe commanding oï¬ƒcer of Camp Dix told us he did not \\nwant it near his soldiers.â€3\\nOther sites considered included Toms River, \\nLakehurst, Lacey, and Hammonton.4 Ultimately, \\nHammonton was selected. Although no de/f_inite source can be cited as to why the Hammonton-Mullica Township site was selected over the other three candidates, major reasons did include the potential quality of town life and the design of a safety zone. Additional testimony from Lieutenant Colonel Hawkins reveals this consideration: â€œWe had to build rather attractive accommodations [at Amatol] . . . to get [people] there. . . .We had to make it just as attractive as we could in order to get people to stay there; particularly in view of the knowledge of most people of the danger.â€\\n5 \\nBeing situated between Atlantic City and Philadelphia, Amatol was conveniently located for ease of access to the Jersey Shore and a great metropolitan city. In addition to town amenities, Lieutenant Colonel Hawkins also explained that the size of Amatolâ€”6,000 acresâ€”was meant as a â€œsubstantial safety zone.â€\\n6\\nC/o.sc/n.sc/s.sc/t.sc/r.sc/u.sc/c.sc/t.sc/i.sc/o.sc/n.sc /a.sc/n.sc/d.sc D/e.sc/v.sc/e.sc/l.sc/o.sc/p.sc/m.sc/e.sc/n.sc/t.sc\\nAfter the company made its /f_inal site determination, \\nthe Atlantic Loading Company immediately went to work, which caught neighboring towns by surprise. /T_he Tuckerton Beacon reported that on March 2, 1918, the residents of Elwood and Hammonton went to bed with a quiet night as if it had been any other. To their surprise, they awoke to see a large force of strangers in their midst, a half hundred freight cars bearing the legend â€œOrdnance Department U.S.A.,â€ and heavy motor trucks noisily chugging in the streets.\\n7 /T_he next day, March 4, \\nconstruction began.\\nA company street, Amatol. From Hammel, Amatol, 293.The 75 mm shell loading plant, Amatol. From Hammel, Amatol, 4.3All Aboard for Amatol\\nIn his work, Construction and Operation of a Shell \\nLoading Plant and the Town of Amatol, New Jersey, \\ncommonly known as the â€œAmatol book,â€ Victor F. Hammel records a description of the construction In total, 1,600 acres had to be cleared for the plant, and 38 acres had to be cleared and 255 acres trimmed for the town.\\n8 /T_his was no easy task. /T_he land was mostly \\nwooded with some acres of swamp and was diï¬ƒcult to clear due to a â€œdense, tangled nature of growth.â€\\n9 /T_he \\ninitial workforce included forty men, and, within the /f_irst week, the /f_irst bunkhouse and mess hall were built.\\n10\\nDevelopment was rapid. By June, the population was \\nnearly 2,000, and train service was added.11 By August, \\nadvertisements for Amatol described it asa city with all modern improvements . . . with \\nelectric lights, a /f_ire department, a police depart-ment, a modern sewage system, a modern water plant, a theatre, a Y.M.C.A, an auditorium, bowl-ing alleys, and other city amenities.\\n12\\nAmatol also promised many employment opportunities, \\nnot just with the munitions plant, but with local businesses, like barbers, tailors, jewelers, watchmakers, and grocers.\\nBy the signing of the armistice, plant and town \\nstructures included 122 loading buildings, 159 storage buildings and magazines, 4 administration buildings, 6 oï¬ƒce buildings, 22 industrial buildings, 7 hospitals,\\nThe Amatol facility occupied 6,000 acres. Above, the loading plant occupies the lefthand property. The workersâ€™ town is located upper right. â€œThe magnitude of the work is indicated by the extent and variety of the utilities; three central steam heating stations, two auxiliary steam-electric generating stations, a 1,750,000 gallon water supply system, a modern sewage disposal system and ultimately 50 miles of standard gauge railroad with 10 locomotives and 30 passenger coaches. â€ Quotation and image from Hammel, Amatol, 13.4SoJourn\\nAmatol, New Jersey. â€œAn example of the art of town planning. With an undeveloped piece of land it was possible to apply the principles of \\nartistic town design and create unites in correct and convenient relation to each other. â€ Quotation and illustration from Hammel, Amatol, 179.\\nEast K Street from Fiftieth Street, Amatol. Postcard courtesy of the Paul W. Schopp collection.5All Aboard for Amatol\\n16 recreation centres, 1 theatre, 18 mess halls, 4 garages, \\n15 warehouses, 21 railroad structures, 12 stables, 130 guardhouses, sentry boxes, searchlights, etc., 21 commercial stores, 98 male dormitories and bunkhouses, 21 female dormitories, 140 miscellaneous buildings, 24 Army barracks, 4 Y.M.C.A.s, 11 individual residences, 33 multiple houses, 227 workmenâ€™s houses, 1 post oï¬ƒce, 1 school house, 2 /f_ire houses, 444 tent tops, 96 two-roomed family apartments, 179 four-roomed family apartments, and 23 three-roomed family apartments.\\n13 \\nOn average, there were four buildings built per day, which was a remarkable speed.\\n14 /T_he /f_inal workforce \\nincluded over 5,000 construction workers.15\\n/T_he town reached a peak population of 7,000,16 had \\nthe capacity to house over 10,000,17 and was planned to \\naccommodate a possible population of 25,000.18\\nA/m.sc/a.sc/t.sc/o.sc/l.sc P/l.sc/a.sc/n.sc/t.sc\\nLoading operations at the Amatol plant began \\non July 31, 1918, and, on August 3, the /f_irst shell was loaded.\\n19 /T_he plant was capable of loading\\n60,000 shells of all sizes, 50,000 boosters, 50,000 \\nhand grenades, and 20,000 ri/f_le grenades per day, as well as components. To accomplish this work there were 15 smokeless-powder magazines, 33 T.N.T. magazines, 49 miscellaneous storage buildings, 642 plant structures in all.\\n20\\n/T_he plant, during its operational lifespan, loaded over \\n9,000,000 rounds of ammunition of various types and sizes.\\n21\\nAlthough T.N.T. was preferred and used when \\navailable, amatol, a mixture of T.N.T. and ammonium nitrate, was used as an alternative due to the shortage of T.N.T. Amatol was â€œthe main charge pressed into the shell (unless straight T.N.T. or a 50/50 mix was used), [and] contains 80 percent of ammonium nitrate and 20 percent of T.N.T.â€.\\n22 /T_he one disadvantage of amatol was \\nthat ammonium nitrate absorbs moisture rapidly, and, in time, deteriorates. /T_he life of a shell loaded with amatol was uncertain after /f_ive years while the life of a shell loaded with T.N.T. was inde/f_inite, and certainly at least 25 years.\\n23\\n/T_he Armyâ€™s Ordnance Department took over the \\nplant in February 1919 and denominated it Amatol Arsenal.\\n24 By 1923, Amatol Arsenal ceased operations.\\nC/a.sc/m.sc/p.sc A/m.sc/a.sc/t.sc/o.sc/l.sc\\nIn October 1918, 2,400 troops arrived at Amatol to \\nassist in operations there.25 /T_heir job was to bring the\\n(Top to bottom) General Dormitory; Interior Swimming Pool; Main Mess Hall; Bank Building and Post Oï¬ƒce, Amatol. Postcards courtesy of the Paul W. Schopp collection.6SoJourn\\n(Top to bottom) Receiving empty \\nshell for loading; the delicate process of pouring T.N.T. into shells; inserting boosters in 75 mm shells;  (bottom left) painting completed rounds; (bottom right) placing completed shells in /f_i  bre containers in preparation for shipping. Photographs from Hammel,  Amatol.7All Aboard for Amatol\\n(Top left) Guard train enroute to T.N.T. section; (top right) Battalion Roll Call; (middle) African American Road-Working Detachment; \\n(bottom left) Canteen; (bottom right) Enlisted Menâ€™s Barracks. Photographs from Construction and Operation of a Shell Loading Plant and the Town of Amatol, New Jersey.8SoJourn\\noutput to full loading capacity and, when necessary, \\nperform civilian tasks. /T_he Philadelphia Inquirer reported that in mid October, many of these soldiers decided to take French leaveâ€”an unauthorized departureâ€”by traveling to Atlantic City. A few of the soldiers were jailed when they were unable to show â€œleave of absence.â€\\n26 By the time of the armistice, there were \\n3,800 oï¬ƒcers and enlisted men at Camp Amatol, as it became known.\\n27\\n/T_here were also Camp Amatol sports teams. \\n Basketball was a popular sport played amongst soldiers, including oï¬ƒcers. A December 6, 1918, news article records, â€œthe Hammonton /f_ive defeated the U.S. Ord-\\nnance quintet from Camp Amatol by the score of 19 to 10.â€\\n28 A December 13, 1918, article describes the U.S. \\nOrdnance Oï¬ƒcers team defeating the Amatol team in basketball by the score of 25 to 10.\\n29\\nC/o.sc/n.sc/c.sc/l.sc/u.sc/s.sc/i.sc/o.sc/n.sc\\nIn just a matter of months after signing the \\nArmistice, the population of Amatol had virtually van-ished, with only a few hundred remaining. /T_he former munitions complex experienced a brief resurgence in 1926 after an investor backed the construction of the Atlantic City Speedway there. /T_he revival was short-lived as interest quickly waned among the visiting public and other investors.\\nLittle remains of Amatol today. After a hike \\nthrough the Pine Barrens, remnants of the munitions plant, town, and racetrack can be found at the former site of Amatol. Of the many buildings and homes constructed, only two still remain: the now- abandoned State Police barracks outside of Hammonton and one house, which was moved to the White Horse Pike, where it is still occupied.\\n30 During the 1980s, the \\nAmatol site was considered for use as a land/f_ill, but the proposal was rejected due to environmental concerns and opposition by local residents.\\n31 Recently, in 2017, \\nmore than 500 acres of Amatol (now the Pine Barrens) were preserved.\\n32\\nAmatol was not the only planned community that \\nSouth Jersey saw built during World War I. Belcoville, Atlantic County, was also built as a munitions plant and village. Yorkship Village (now Fairview), Camden County, was built to house shipyard workers and their families. In addition, America also saw many new mil-\\nitary bases built, which included Camp Dix (now Fort Dix), Burlington County.\\nWhat was the overall impact of Amatol on the sur-\\nrounding area? /T_here is no reliable and readily-available information to answer this question. Where did the res-idents of Amatol go once operations ceased? According \\n(Top) 75 mm shell component parts; (bottom) Mark III Drop Bomb. Both munitions were loaded at Amatol. Photographs from Construction and Operation of a Shell Loading Plant and the Town of Amatol, New Jersey.9All Aboard for Amatol\\nto the Press of Atlantic City, â€œMany of Amatolâ€™s laborers, \\npredominantly Irish and Polish from Philadelphia, stayed in the area and started families.â€\\n33\\nAs the centennial of Americaâ€™s participation in the \\nFirst World War draws to a close, it is good to take a moment to remember a signi/f_icant town, built in a forest in New Jersey, which helped with the war eï¬€ort and provided employment for thousands of American workers.\\nA/b.sc/o.sc/u.sc/t.sc /t.sc/h.sc/e.sc A/u.sc/t.sc/h.sc/o.sc/r.sc\\nDaniel J. Dinnebeil completed his Master of Arts in \\nAmerican Studies at Stockton University in Fall 2018. He also received his BA in Economics from Stockton in 2015. Having served in the United States Coast Guard as an Electronics Technician, Daniel has an abiding interest in military history.\\nE/n.sc/d.sc/n.sc/o.sc/t.sc/e.sc/s.sc\\n1 Victor F. Hammel, Construction and Operation of a Shell-\\nLoading Plant and the Town of Amatol, New Jersey: For the \\nUnited States Government Ordnance Department, U.S. Army (New York: Atlantic Loading Company, 1918), 14.\\n2 â€œLocate War Plant at Toms River,â€ Asbury Park Press, \\nFebruary 21, 1918.\\n3War Expenditures: Hearings Before Subcommittee No. 5 (Ordnance) of the Select Committee on Expenditures in the War Department, House of Representatives, Sixty-sixth Congress on War Expenditures, vol. 1 (Washington: Government Printing Oï¬ƒce, 1919), 491.\\n4 â€œLocate War Plant at Toms River,â€ Asbury Park Press, \\nFebruary 21, 1918.\\n5War Expenditures, 511.\\n6War Expenditures, 493.\\n7 â€œPeople Arise to Discover Evidence of Governmentâ€™s War \\nActivities,â€ Tuckerton Beacon, March 7, 1918.\\n8 Hammel, Construction and Operation, 16.9 Ibid.10 Ibid.11 â€œAccommodations for Loaders,â€ Philadelphia Inquirer, June \\n5, 1918.\\n12 â€œAmatol [Advertisement],â€ Philadelphia Inquirer, August \\n25, 1918.\\n13 Hammel, Construction and Operation, 17â€“18.14 Ibid., 18.15 Ibid., 25.16Forty-/T_hird Annual Report of the Department of Health of the State of New Jersey (Trenton: MacCrellish & Quigley, 1912) 175.\\n17 Hammel, Construction and Operation, 1618 Ibid., 181.19 Ibid., 16.(Left to right) Attractive workerâ€™s home with enclosed porch and sun parlor; Corner of a typical living room; Four-room two-family cottage. Photographs from Construction and Operation of a Shell Loading Plant and the Town of Amatol, New Jersey, 189, 195.\\nPostcard photograph of private Joseph Straley, Military Police, Amatol, New Jersey. Courtesy of the Paul W. Schopp collection.10SoJourn\\nGovernment Printing Oï¬ƒ   ce, 1920), 111.\\n28 â€œHammonton Defeats Ordnance Five,â€ Philadelphia \\nInquirer, December 6, 1918.\\n29 Philadelphia Inquirer, December 13, 1918.\\n30 Wallace McKelvey, â€œMullica Township Ghost Town \\nAmatol was Once Munitions Village,â€ Press of Atlantic City, \\nDecember 16, 2013.\\n31 McKelvey, â€œMullica Township Ghost Town Amatol.â€\\n32 Michelle Brunetti, â€œMullicaâ€™s Amatol Site Part of Recent \\nPreservation in Pinelands,â€ Press of Atlantic City, October 14, 2017.\\n33 McKelvey, â€œMullica Township Ghost Town Amatol.â€20 /T_h  e Ordnance Districts, 1918â€“1919 (Washington: Government Printing Oï¬ƒ   ce, 1920), 110.\\n21 Hammel, Construction and Operation, 61.22 Ibid., 135.23 Ibid, 136.24 War Department, Annual Reports, 1919 (Washington: Government Printing Oï¬ƒ   ce, 1920), 3937.\\n25 â€œTroops Taking Over Loading Plant,â€ Philadelphia Inquirer, \\nOctober 10, 1918.\\n26 â€œ3000 Soldiers Reach Shell Loading Plants: Scores Take \\nFrench Leave and Visit Seaside Resort, Housing Problem,â€ Philadelphia Inquirer, October 14, 1918.\\n27 /T_h  e Ordnance Districts, 1918â€“1919 (Washington: \\nJunction where Camden, Atlantic City, and Amatol Railroad trains meet. Photographs from Hammel, Amatol, 271.']\n"
          ]
        }
      ],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Suggestions to My Readers\\n\\nT have written this book to share with you, my readers, that which\\n1 know and teach. It incorporates portions of my little blue book,\\nHeal Your Body, which has become widely accepted as an authorita-\\ntive work on the mental patterns that create dis-eases in the body.\\n\\n1 have had hundreds of letters from readers asking me to share\\nmore of my information. Many persons who have worked with me\\nas private clients, and those who have taken my workshops here and\\nabroad, have requested I take the time to write this book.\\n\\nI have set up this book to take you through a session, just as I\\nwould if you came to me as a private client and attended one of my\\nworkshops.\\n\\nIf you will do the exercises progressively as they appear in the\\nbook, by the time you have finished, you will have begun to change\\nyour life.\\n\\nI suggest you read through the book once. Then slowly read it\\nagain, only this time do each exercise in depth. Give yourself time to\\nwork with each one.\\n\\nIf you can,']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### â“Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### â“Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ow\\nare the keys to positive changes.\\n\\nWhen we really love ourselves, everything in our life works.\\n\\n \\n\\x0c\\\\\\n\\nIn the infinity of life where I am, all is perfect,\\nwhole, and complete, and yet life is ever changing.\\nThere is no beginning and no end,\\nonfy a constant cycling and recycling\\nof substance and experiences.\\n\\nLife is never stuck or static or stale,\\nfor each moment is ever new and fresh.\\n\\nTam one with the very Power that created me, and this Power\\nhas given me the power to create my own circumstances.\\nI rejoice in the knowledge that I have the power\\nof my own mind to use in any way I choose.\\nEvery moment of life is a new beginning point\\nas we move from the old. This moment is a new point\\nof beginning for me right here and right now.\\n\\nAll is well in my world.\\n\\n \\n\\x0c',\n",
              "  np.float64(0.42061856767184186)),\n",
              " (' come from or how lowly it\\nwas, we can totally change our lives for the better.\\n\\nKnow that when you work with these ideas, my loving support is\\n\\nwith you.\\n\\n \\n\\x0cSome Points\\nof My Philosophy\\nWe are each responsible for all of our experiences.\\nEvery thought we think is creating our future.\\nThe point of power is always in the present moment.\\n\\nEveryone suffers from self-hatred and guilt.\\n\\nThe bottom line for everyone is,\\nâ€œTm not good enough.â€\\n\\nIts only a thought, and a thought can be changed.\\nWe create every so-called illness in our body.\\n\\nResentment, criticism, and guilt\\nare the most damaging patterns.\\n\\nReleasing resentment will dissolve even cancer.\\nWe must release the past and forgive everyone.\\nWe must be willing to begin to learn to love ourselves.\\n\\nSelf-approval and self-acceptance in the now\\nare the keys to positive changes.\\n\\nWhen we really love ourselves, everything in our life works.\\n\\n \\n\\x0c\\\\\\n\\nIn the infinity of life where I am, all is perfect,\\nwhole, and complete, and yet life is ever ',\n",
              "  np.float64(0.35141698918631536)),\n",
              " ('l have begun to change\\nyour life.\\n\\nI suggest you read through the book once. Then slowly read it\\nagain, only this time do each exercise in depth. Give yourself time to\\nwork with each one.\\n\\nIf you can, work through the exercises with a friend or with a\\nmember of your family.\\n\\nEach chapter opens with an affirmation. Each of these is good to\\nuse when you are working on that area of your life. Take two or three\\ndays to study and work with each chapter. Keep saying and writing\\nthe affirmation that opens the chapter.\\n\\n \\n\\x0cYOU CAN HEAL YOUR LIFE\\n\\nThe chapters close with a treatment. This is a flow of positive\\nideas designed to change consciousness. Read over this treatment\\nseveral times a day.\\n\\nI close this book by sharing with you my own story. I know it will\\nshow you that no matter where we have come from or how lowly it\\nwas, we can totally change our lives for the better.\\n\\nKnow that when you work with these ideas, my loving support is\\n\\nwith you.\\n\\n \\n\\x0cSome Points\\nof My Philosophy\\nWe are each ',\n",
              "  np.float64(0.25576745550401386))]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What is the infinity of life?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the modelâ€™s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best way to write a loop depends on your specific use case, but I'll provide a general overview of how to write loops in Python effectively, along with some best practices!\n",
            "\n",
            "### Common Types of Loops in Python\n",
            "\n",
            "1. **For Loop**:\n",
            "   This is useful when you want to iterate over a sequence (like a list, tuple, or string).\n",
            "\n",
            "   ```python\n",
            "   # Example: Print numbers from 1 to 5\n",
            "   for i in range(1, 6):\n",
            "       print(i)\n",
            "   ```\n",
            "\n",
            "2. **While Loop**:\n",
            "   This is best used when the number of iterations isn't known beforehand and is determined by a condition.\n",
            "\n",
            "   ```python\n",
            "   # Example: Print numbers from 1 to 5\n",
            "   i = 1\n",
            "   while i <= 5:\n",
            "       print(i)\n",
            "       i += 1\n",
            "   ```\n",
            "\n",
            "### Best Practices for Writing Loops\n",
            "\n",
            "- **Keep it Simple**: Ensure that your loop is easy to read and understand. Avoid complex nested loops where possible.\n",
            "  \n",
            "- **Use `enumerate` for Indexes**: When you need both the index and the value from a list, use `enumerate()`.\n",
            "\n",
            "  ```python\n",
            "  fruits = ['apple', 'banana', 'cherry']\n",
            "  for index, fruit in enumerate(fruits):\n",
            "      print(index, fruit)\n",
            "  ```\n",
            "\n",
            "- **List Comprehensions**: For simple transformations, consider using list comprehensions for a more Pythonic approach.\n",
            "\n",
            "  ```python\n",
            "  # Example: Create a list of squares\n",
            "  squares = [x**2 for x in range(1, 6)]\n",
            "  print(squares)\n",
            "  ```\n",
            "\n",
            "- **Avoid Changing the List While Iterating**: If you need to modify a list, itâ€™s best to create a copy or use a list comprehension.\n",
            "\n",
            "- **Use `break` and `continue` Wisely**: These can help control the flow of your loop, but overusing them can lead to confusion.\n",
            "\n",
            "  ```python\n",
            "  for number in range(10):\n",
            "      if number == 5:\n",
            "          continue  # Skip this iteration\n",
            "      print(number)\n",
            "  ```\n",
            "\n",
            "- **Consider Performance**: If you're dealing with a large dataset, look for ways to optimize your loop, perhaps by using built-in functions or libraries like NumPy for heavy computational tasks.\n",
            "\n",
            "By keeping these points in mind, you can write efficient and readable loops in Python. If you have a specific scenario or code snippet youâ€™d like to discuss, feel free to share!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses {response_style} and {response_length}\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Number of relevant sources found: {context_count}\n",
        "{similarity_scores}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(\n",
        "    RAG_SYSTEM_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"response_style\": \"concise\",\n",
        "        \"response_length\": \"brief\"\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_user_prompt = UserRolePrompt(\n",
        "    RAG_USER_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"context_count\": \"\",\n",
        "        \"similarity_scores\": \"\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, \n",
        "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.response_style = response_style\n",
        "        self.include_scores = include_scores\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
        "        # Retrieve relevant contexts\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "        \n",
        "        context_prompt = \"\"\n",
        "        similarity_scores = []\n",
        "        \n",
        "        for i, (context, score) in enumerate(context_list, 1):\n",
        "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
        "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
        "        \n",
        "        # Create system message with parameters\n",
        "        system_params = {\n",
        "            \"response_style\": self.response_style,\n",
        "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
        "        }\n",
        "        \n",
        "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
        "        \n",
        "        user_params = {\n",
        "            \"user_query\": user_query,\n",
        "            \"context\": context_prompt.strip(),\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
        "        }\n",
        "        \n",
        "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
        "            \"context\": context_list,\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
        "            \"prompts_used\": {\n",
        "                \"system\": formatted_system_prompt,\n",
        "                \"user\": formatted_user_prompt\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The philosophy of the infinity of life, as outlined in the provided context, emphasizes several key points:\n",
            "\n",
            "1. **Perfection of Life:** The philosophy posits that \"in the infinity of life where I am, all is perfect, whole, and complete,\" indicating a belief in the inherent perfection of existence despite its ever-changing nature.\n",
            "\n",
            "2. **Constant Change:** It notes that life is \"ever changing\" and describes it as a \"constant cycling and recycling of substance and experiences,\" reinforcing the idea that life is dynamic and fluid rather than static.\n",
            "\n",
            "3. **Power of Creation:** The philosophy states, \"I am one with the very Power that created me, and this Power has given me the power to create my own circumstances.\" This highlights the belief in individual empowerment and the ability to shape one's own life through thought and action.\n",
            "\n",
            "4. **Mind's Role in Life:** There is a strong emphasis on the mind, suggesting that \"I rejoice in the knowledge that I have the power of my own mind to use in any way I choose.\" This reflects a philosophy that underscores the significance of mindset in influencing oneâ€™s experiences.\n",
            "\n",
            "5. **New Beginnings:** The text asserts that \"every moment of life is a new beginning point,\" encouraging the idea that each moment offers the opportunity for renewal and change.\n",
            "\n",
            "6. **Affirmative Perspective:** The affirmation \"All is well in my world\" encapsulates an optimistic outlook inherent in this philosophy, suggesting an acceptance of one's current state as well as the potential for improvement.\n",
            "\n",
            "These elements collectively illustrate a philosophy that embraces the idea of life as a perfect, continuous journey, driven by individual consciousness and the power of thought.\n",
            "\n",
            "Context Count: 8\n",
            "Similarity Scores: ['Source 1: 0.454', 'Source 2: 0.451', 'Source 3: 0.335', 'Source 4: 0.200', 'Source 5: 0.075', 'Source 6: 0.068', 'Source 7: 0.058', 'Source 8: 0.056']\n"
          ]
        }
      ],
      "source": [
        "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "result = rag_pipeline.run_pipeline(\n",
        "    \"What is the philosophy of the infinity of life?\",\n",
        "    k=8,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"\\nContext Count: {result['context_count']}\")\n",
        "print(f\"Similarity Scores: {result['similarity_scores']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### â“ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through [\"Accessing GPT-3.5-turbo Like a Developer\"](https://colab.research.google.com/drive/1mOzbgf4a2SP5qQj33ZxTz2a01-5eXqk2?usp=sharing) for an answer to this question if you get stuck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
        "\n",
        "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
