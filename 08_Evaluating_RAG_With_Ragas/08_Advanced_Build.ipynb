{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b11fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cada82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 269 documents\n",
      "Application and Verification Guide\n",
      "Introduction\n",
      "This guide is intended for college financial aid administrators and counselors who help students with the financial aid\n",
      "process4completing the Free Application for Federal Student Aid (FAFSAÂ®) form, verifying information, and making\n",
      "corrections and other changes to the information reported on the FAFSA form.\n",
      "Throughout the Federal Student Aid Handbook, we use <college,= <school,= and <institution= interchangeably unless a\n",
      "more specific use is given\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "# Define the path to the document directory\n",
    "path = \"data/\"\n",
    "\n",
    "# Load all .pdf documents using PyMuPDFLoader\n",
    "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "# Optional: Check number of documents and preview content\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(docs[0].page_content[:500])  # Preview first 500 characters of first doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d61f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DJ/08_advanced_build/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]           unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "Applying SummaryExtractor:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:24<00:32,  3.22s/it]Property 'summary' already exists in node '19bfc8'. Skipping!\n",
      "Applying SummaryExtractor:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:28<00:31,  3.45s/it]Property 'summary' already exists in node 'eb9bf4'. Skipping!\n",
      "Applying SummaryExtractor:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:29<00:22,  2.81s/it]Property 'summary' already exists in node 'ee8901'. Skipping!\n",
      "Applying SummaryExtractor:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:29<00:15,  2.21s/it]Property 'summary' already exists in node '2893f7'. Skipping!\n",
      "Applying SummaryExtractor:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:29<00:10,  1.71s/it]Property 'summary' already exists in node '606283'. Skipping!\n",
      "Applying SummaryExtractor:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:36<00:15,  3.01s/it]Property 'summary' already exists in node 'fd8f36'. Skipping!\n",
      "Property 'summary' already exists in node '8d092e'. Skipping!\n",
      "Applying SummaryExtractor:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:43<00:09,  3.28s/it]Property 'summary' already exists in node '05a907'. Skipping!\n",
      "Applying SummaryExtractor:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:50<00:03,  3.33s/it]Property 'summary' already exists in node '96009a'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  26%|â–ˆâ–ˆâ–Œ       | 16/61 [00:01<00:03, 14.11it/s]Property 'summary_embedding' already exists in node 'eb9bf4'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '05a907'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '96009a'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '2893f7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '606283'. Skipping!\n",
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 23/61 [00:01<00:01, 21.31it/s]Property 'summary_embedding' already exists in node 'fd8f36'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'ee8901'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '8d092e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '19bfc8'. Skipping!\n",
      "Generating personas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.21it/s]                                           \n",
      "Generating Scenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  3.00s/it]\n",
      "Generating Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:25<00:00,  2.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the main impact of the Fostering Undergraduate Talent by Unlocking Resources for Education Act on the FAFSA process?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Wrap OpenAI LLM and embedding model for use with RAGAS\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "# Initialize test set generator\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# Generate a synthetic dataset from your docs (limit to first 20 docs for speed)\n",
    "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)\n",
    "\n",
    "# Optional: preview first generated question\n",
    "dataset.samples[0].eval_sample.user_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda99296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main impact of the Fostering Under...</td>\n",
       "      <td>[Application and Verification Guide Introducti...</td>\n",
       "      <td>The Fostering Undergraduate Talent by Unlockin...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wut happend to the retuning FAFSA filers secton?</td>\n",
       "      <td>[Chapter 1: The Application Process We removed...</td>\n",
       "      <td>The &lt;Returning FAFSA Filers= section was remov...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why FPS look at the FAFSA and say there mistak...</td>\n",
       "      <td>[The FPS also checks the application for possi...</td>\n",
       "      <td>The FPS checks the application for inconsisten...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how i get isir for student with fafsa partner ...</td>\n",
       "      <td>[Output Documents After processing is complete...</td>\n",
       "      <td>if your school not listed on student fafsa, yo...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how do the fps output documents like isir and ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe FPS also checks the applicatio...</td>\n",
       "      <td>the fps output documents, which are the isir a...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How has the FAFSA application process changed ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nApplication and Verification Guide...</td>\n",
       "      <td>The FAFSA application process has changed in t...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If a student submits a FAFSA application with ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe FPS also checks the applicatio...</td>\n",
       "      <td>When a student submits a FAFSA application wit...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Howw does the disclosure and use of Federal Ta...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2. The disclosure of their FTI by ...</td>\n",
       "      <td>The disclosure and use of Federal Tax Informat...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>According to Chapter 2, how does the determina...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAVG, Chapter 2, Example 6: A stude...</td>\n",
       "      <td>In Chapter 2, the determination of legal depen...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wich departmant is responsibel for obtaning co...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nFederal Tax Information The follow...</td>\n",
       "      <td>Only the Department has the authority to obtai...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>if a student ISIR say no to having dependents ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe student was born before Januar...</td>\n",
       "      <td>if a student ISIR say no to having dependents ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What specific federal tax information does the...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nFederal Tax Information The follow...</td>\n",
       "      <td>The Department receives specific federal tax i...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What is the main impact of the Fostering Under...   \n",
       "1    Wut happend to the retuning FAFSA filers secton?   \n",
       "2   Why FPS look at the FAFSA and say there mistak...   \n",
       "3   how i get isir for student with fafsa partner ...   \n",
       "4   how do the fps output documents like isir and ...   \n",
       "5   How has the FAFSA application process changed ...   \n",
       "6   If a student submits a FAFSA application with ...   \n",
       "7   Howw does the disclosure and use of Federal Ta...   \n",
       "8   According to Chapter 2, how does the determina...   \n",
       "9   Wich departmant is responsibel for obtaning co...   \n",
       "10  if a student ISIR say no to having dependents ...   \n",
       "11  What specific federal tax information does the...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [Application and Verification Guide Introducti...   \n",
       "1   [Chapter 1: The Application Process We removed...   \n",
       "2   [The FPS also checks the application for possi...   \n",
       "3   [Output Documents After processing is complete...   \n",
       "4   [<1-hop>\\n\\nThe FPS also checks the applicatio...   \n",
       "5   [<1-hop>\\n\\nApplication and Verification Guide...   \n",
       "6   [<1-hop>\\n\\nThe FPS also checks the applicatio...   \n",
       "7   [<1-hop>\\n\\n2. The disclosure of their FTI by ...   \n",
       "8   [<1-hop>\\n\\nAVG, Chapter 2, Example 6: A stude...   \n",
       "9   [<1-hop>\\n\\nFederal Tax Information The follow...   \n",
       "10  [<1-hop>\\n\\nThe student was born before Januar...   \n",
       "11  [<1-hop>\\n\\nFederal Tax Information The follow...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   The Fostering Undergraduate Talent by Unlockin...   \n",
       "1   The <Returning FAFSA Filers= section was remov...   \n",
       "2   The FPS checks the application for inconsisten...   \n",
       "3   if your school not listed on student fafsa, yo...   \n",
       "4   the fps output documents, which are the isir a...   \n",
       "5   The FAFSA application process has changed in t...   \n",
       "6   When a student submits a FAFSA application wit...   \n",
       "7   The disclosure and use of Federal Tax Informat...   \n",
       "8   In Chapter 2, the determination of legal depen...   \n",
       "9   Only the Department has the authority to obtai...   \n",
       "10  if a student ISIR say no to having dependents ...   \n",
       "11  The Department receives specific federal tax i...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "dataset_naive = copy.deepcopy(dataset)\n",
    "dataset_semantic_k5 = copy.deepcopy(dataset) \n",
    "dataset_semantic_k20 = copy.deepcopy(dataset)\n",
    "dataset_rerank = copy.deepcopy(dataset)\n",
    "\n",
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058049bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1102\n",
      "Application and Verification Guide\n",
      "Introduction\n",
      "This guide is intended for college financial aid administrators and counselors who help students with the financial aid\n",
      "process4completing the Free Application for Federal Student Aid (FAFSAÂ®) form, verifying information, and making\n",
      "corrections and oth\n"
     ]
    }
   ],
   "source": [
    "# Naive Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Naive chunking: split docs into 1000-character chunks with 200-character overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Optional: Confirm result\n",
    "print(f\"Total chunks created: {len(split_documents)}\")\n",
    "print(split_documents[0].page_content[:300])  # Preview first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b3fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Define the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Set up Qdrant in-memory client\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create a Qdrant collection (1536 = embedding size of text-embedding-3-small)\n",
    "client.create_collection(\n",
    "    collection_name=\"loan_data\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Store chunks in Qdrant\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"loan_data\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "_ = vector_store.add_documents(documents=split_documents)\n",
    "\n",
    "# Expose retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906a4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph Pipeline\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the RAG prompt template\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context.\n",
    "You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# Define the LLM for generation\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "\n",
    "# Retrieval function\n",
    "def retrieve(state):\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Generation function\n",
    "def generate(state):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = rag_prompt.format_messages(\n",
    "        question=state[\"question\"],\n",
    "        context=docs_content\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb6e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define the state schema\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "# Build the LangGraph pipeline\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4f2d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context mentions different aspects of student loans, such as Direct Loans and various loan terms, but it does not explicitly list or define the different kinds of loans. Therefore, based solely on the provided information, there is no specific answer regarding the types of loans.\n"
     ]
    }
   ],
   "source": [
    "# Test Run\n",
    "response = graph.invoke({\"question\": \"What are the different kinds of loans?\"})\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215bcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset_naive:\n",
    "    # Run LangGraph with the question\n",
    "    response = graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "\n",
    "    # Save the generated response and retrieved context\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [\n",
    "        doc.page_content for doc in response[\"context\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c2a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample generated answer:\n",
      "Question: What is the main impact of the Fostering Undergraduate Talent by Unlocking Resources for Education Act on the FAFSA process?\n",
      "Answer: The main impact of the Fostering Undergraduate Talent by Unlocking Resources for Education (FUTURE) Act on the FAFSA process is the implementation of the FUTURE Act Direct Data Exchange (FA-DDX), which allows the IRS to directly disclose certain tax information to the Department of Education. This eliminates the need for most applicants to self-report their income and tax information when completing the FAFSA form.\n",
      "Context snippet: analysis, and many policies and procedures for schools that participate in the Title IV programs. FSA implemented the\n",
      "FAFSA Simplification Act alongside the FAFSA portion of the Fostering Undergraduat\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… Sample generated answer:\")\n",
    "print(\"Question:\", dataset_naive.samples[0].eval_sample.user_input)\n",
    "print(\"Answer:\", dataset_naive.samples[0].eval_sample.response)\n",
    "print(\"Context snippet:\", dataset_naive.samples[0].eval_sample.retrieved_contexts[0][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd66f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [05:10<00:00,  4.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8554, 'faithfulness': 0.9335, 'factual_correctness(mode=f1)': 0.6183, 'answer_relevancy': 0.7784, 'context_entity_recall': 0.5048, 'noise_sensitivity(mode=relevant)': 0.1974}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.evaluation import EvaluationDataset\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    ResponseRelevancy,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "\n",
    "# Wrap the evaluator LLM (same or smaller model is fine)\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
    "\n",
    "# Convert dataset to RAGAS EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(dataset_naive.to_pandas())\n",
    "\n",
    "# Optional: set timeout to avoid stalling on any row\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall(),\n",
    "        NoiseSensitivity()\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "\n",
    "# Display results\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729377ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/DJ/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/DJ/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b846e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def semantic_chunk_document(\n",
    "    doc: Document,\n",
    "    similarity_threshold: float = 0.85,\n",
    "    max_chunk_size: int = 1000\n",
    ") -> List[Document]:\n",
    "    sentences = nltk.sent_tokenize(doc.page_content)\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    sentence_embeddings = embedding_model.embed_documents(sentences)\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_length = len(sentences[0])\n",
    "    current_vector = sentence_embeddings[0]\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity(\n",
    "            [current_vector],\n",
    "            [sentence_embeddings[i]]\n",
    "        )[0][0]\n",
    "        sentence_len = len(sentences[i])\n",
    "\n",
    "        if sim >= similarity_threshold and current_length + sentence_len <= max_chunk_size:\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_length += sentence_len\n",
    "            current_vector = np.mean(\n",
    "                [embedding_model.embed_query(\" \".join(current_chunk))], axis=0\n",
    "            )\n",
    "        else:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunks.append(Document(page_content=chunk_text, metadata=doc.metadata))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_length = sentence_len\n",
    "            current_vector = sentence_embeddings[i]\n",
    "\n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = \" \".join(current_chunk)\n",
    "        chunks.append(Document(page_content=chunk_text, metadata=doc.metadata))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530a5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunks = []\n",
    "for doc in docs:\n",
    "    semantic_chunks.extend(semantic_chunk_document(doc))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0ea93cd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96426a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbd05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fe9552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4630 semantic chunks.\n",
      "Application and Verification Guide\n",
      "Introduction\n",
      "This guide is intended for college financial aid administrators and counselors who help students with the financial aid\n",
      "process4completing the Free Application for Federal Student Aid (FAFSAÂ®) form, verifying information, and making\n",
      "corrections and other changes to the information reported on the FAFSA form.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated {len(semantic_chunks)} semantic chunks.\")\n",
    "print(semantic_chunks[0].page_content[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4264f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Reuse or redefine the embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a new in-memory Qdrant client\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# New collection name for semantic chunks\n",
    "client.create_collection(\n",
    "    collection_name=\"loan_data_semantic\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Add semantic chunks to the collection\n",
    "semantic_vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"loan_data_semantic\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "_ = semantic_vectorstore.add_documents(documents=semantic_chunks)\n",
    "\n",
    "# Expose a retriever\n",
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5de1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define a New LangGraph for Semantic Chunking\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Prompt stays the same\n",
    "semantic_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# Semantic retrieval function\n",
    "def semantic_retrieve(state):\n",
    "    retrieved_docs = semantic_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Generation function stays the same\n",
    "def generate(state):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = semantic_prompt.format_messages(\n",
    "        question=state[\"question\"], context=docs_content\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "# Shared state schema\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "# Build semantic graph\n",
    "semantic_graph_builder = StateGraph(State).add_sequence([semantic_retrieve, generate])\n",
    "semantic_graph_builder.add_edge(START, \"semantic_retrieve\")\n",
    "semantic_graph = semantic_graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ad9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset_semantic_k5:\n",
    "    response = semantic_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [\n",
    "        doc.page_content for doc in response[\"context\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9693a8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [03:56<00:00,  3.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.5478, 'faithfulness': 0.7615, 'factual_correctness(mode=f1)': 0.6700, 'answer_relevancy': 0.8553, 'context_entity_recall': 0.4344, 'noise_sensitivity(mode=relevant)': 0.1552}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.evaluation import EvaluationDataset\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    ResponseRelevancy,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "\n",
    "# Wrap evaluator\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
    "\n",
    "# Convert dataset to EvaluationDataset\n",
    "evaluation_dataset_semantic = EvaluationDataset.from_pandas(dataset_semantic_k5.to_pandas())\n",
    "\n",
    "# Run evaluation\n",
    "semantic_result = evaluate(\n",
    "    dataset=evaluation_dataset_semantic,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall(),\n",
    "        NoiseSensitivity()\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=RunConfig(timeout=360)\n",
    ")\n",
    "\n",
    "semantic_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7bd6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa17ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt (reuse previous)\n",
    "semantic_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# New retriever logic\n",
    "def semantic_retrieve(state):\n",
    "    retrieved_docs = semantic_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Generation logic\n",
    "def generate(state):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = semantic_prompt.format_messages(\n",
    "        question=state[\"question\"], context=docs_content\n",
    "    )\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "# Build updated LangGraph\n",
    "semantic_graph_k20 = StateGraph(State).add_sequence([semantic_retrieve, generate])\n",
    "semantic_graph_k20.add_edge(START, \"semantic_retrieve\")\n",
    "semantic_graph_k20 = semantic_graph_k20.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2071b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset_semantic_k20:\n",
    "    response = semantic_graph_k20.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [\n",
    "        doc.page_content for doc in response[\"context\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa66cb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 65/72 [05:19<05:34, 47.81s/it]Exception raised in Job[17]: TimeoutError()\n",
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 66/72 [05:42<04:01, 40.31s/it]Exception raised in Job[29]: TimeoutError()\n",
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 67/72 [05:49<02:31, 30.33s/it]Exception raised in Job[41]: TimeoutError()\n",
      "Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 68/72 [06:02<01:40, 25.17s/it]Exception raised in Job[47]: TimeoutError()\n",
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 69/72 [06:07<00:57, 19.14s/it]Exception raised in Job[53]: TimeoutError()\n",
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 70/72 [06:21<00:35, 17.66s/it]Exception raised in Job[65]: TimeoutError()\n",
      "Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 71/72 [06:52<00:21, 21.59s/it]Exception raised in Job[71]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [07:05<00:00,  5.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.7794, 'faithfulness': 0.8369, 'factual_correctness(mode=f1)': 0.7017, 'answer_relevancy': 0.9258, 'context_entity_recall': 0.4364, 'noise_sensitivity(mode=relevant)': 0.1533}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.evaluation import EvaluationDataset\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    ResponseRelevancy,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "\n",
    "evaluation_dataset_semantic_k20 = EvaluationDataset.from_pandas(dataset_semantic_k20.to_pandas())\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
    "\n",
    "semantic_result_k20 = evaluate(\n",
    "    dataset=evaluation_dataset_semantic_k20,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall(),\n",
    "        NoiseSensitivity()\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=RunConfig(timeout=360)\n",
    ")\n",
    "\n",
    "semantic_result_k20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2044b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass(\"Cohere API Key:\")\n",
    "\n",
    "import cohere\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.documents import Document\n",
    "from typing import TypedDict, List\n",
    "\n",
    "# Set API keys\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])   # or use getpass() for security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c145fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    retrieved_context: List[Document]\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05272873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_rerank(query: str, docs: list, top_n: int = 8):\n",
    "    response = co.rerank(\n",
    "        model=\"rerank-v3.5\",  # or rerank-english-v3.0 if needed\n",
    "        query=query,\n",
    "        documents=[doc.page_content for doc in docs],\n",
    "        top_n=top_n\n",
    "    )\n",
    "    return [docs[r.index] for r in response.results]\n",
    "\n",
    "def retrieve_with_cohere_rerank(state: State):\n",
    "    initial_docs = semantic_vectorstore.similarity_search(state[\"question\"], k=25)\n",
    "    reranked_docs = cohere_rerank(state[\"question\"], initial_docs, top_n=8)\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"retrieved_context\": reranked_docs\n",
    "    }\n",
    "\n",
    "retriever_node = RunnableLambda(retrieve_with_cohere_rerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6435c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "You must answer the question using only the provided context.\n",
    "If you do not know, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\")\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    context_str = \"\\n\\n\".join([doc.page_content for doc in state[\"retrieved_context\"]])\n",
    "    response = llm.invoke(prompt.format(context=context_str, question=state[\"question\"]))\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"retrieved_context\": state[\"retrieved_context\"],\n",
    "        \"answer\": response.content\n",
    "    }\n",
    "\n",
    "llm_node = RunnableLambda(generate_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5aaeb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"retriever\", retriever_node)\n",
    "graph.add_node(\"llm\", llm_node)\n",
    "\n",
    "graph.set_entry_point(\"retriever\")\n",
    "graph.add_edge(\"retriever\", \"llm\")\n",
    "graph.set_finish_point(\"llm\")\n",
    "\n",
    "rerank_chain = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a745ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset_rerank:\n",
    "    response = rerank_chain.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"answer\"]  # Note: rerank uses \"answer\" while others use \"response\"\n",
    "    test_row.eval_sample.retrieved_contexts = [\n",
    "        doc.page_content for doc in response[\"retrieved_context\"]  # Note: rerank uses \"retrieved_context\" while others use \"context\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a43af6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [04:42<00:00,  3.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.5978, 'faithfulness': 0.5556, 'factual_correctness(mode=f1)': 0.4950, 'answer_relevancy': 0.6250, 'context_entity_recall': 0.5267, 'noise_sensitivity(mode=relevant)': 0.0492}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.evaluation import EvaluationDataset\n",
    "from ragas import evaluate, RunConfig\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    ResponseRelevancy,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "\n",
    "evaluation_dataset_rerank = EvaluationDataset.from_pandas(dataset_rerank.to_pandas())\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
    "\n",
    "rerank_result = evaluate(\n",
    "    dataset=evaluation_dataset_rerank,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        ResponseRelevancy(),\n",
    "        ContextEntityRecall(),\n",
    "        NoiseSensitivity()\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=RunConfig(timeout=360)\n",
    ")\n",
    "\n",
    "rerank_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9897e",
   "metadata": {},
   "source": [
    "# Interpretation of Results\n",
    "\n",
    "### ðŸ“Š Chunk Count Comparison: Naive vs. Semantic\n",
    "\n",
    "| Chunking Strategy | Total Chunks | Description |\n",
    "|-------------------|--------------|-------------|\n",
    "| **Naive Chunking** | 1,102        | Fixed-size chunks (1,000 characters with 200-character overlap). Chunks are larger, may split or mix unrelated sentences. |\n",
    "| **Semantic Chunking** | 4,630        | Sentence-level greedy merging based on semantic similarity. Results in smaller, more coherent chunks. |\n",
    "\n",
    "The semantic chunking strategy produces **over 4x** as many chunks as naive chunking. This reflects its finer granularity and semantic awareness. While it increases the number of embeddings, it enhances context relevance and retrieval precision â€” key factors for improving downstream RAG performance.\n",
    "\n",
    "The naive chunking strategy produced 1,102 chunks, while the semantic chunking strategy produced 4,630 chunks â€” more than 4x the number of segments.\n",
    "\n",
    "This difference highlights a key behavior:\n",
    "\n",
    "Naive chunking uses fixed-length blocks (e.g., 1,000 characters), which results in larger, more uniform chunks that often combine unrelated sentences or split thoughts mid-way.\n",
    "\n",
    "Semantic chunking, in contrast, splits text based on sentence boundaries and meaning, leading to shorter, more focused chunks. These are smaller in size but more coherent in content, especially useful for RAG retrieval precision.\n",
    "\n",
    "This trade-off has implications for:\n",
    "\n",
    "Retrieval granularity (semantic chunks allow finer filtering)\n",
    "\n",
    "Context relevance (retrieved passages are more focused)\n",
    "\n",
    "Embedding volume (more chunks = more embeddings = more token usage upfront)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2929e77",
   "metadata": {},
   "source": [
    "## ðŸ“Š RAG Evaluation Results Comparison\n",
    "\n",
    "| **Metric**                       | **Naive** | **Semantic (k=5)** | **Semantic (k=20)** | **Cohere Rerank** |\n",
    "|----------------------------------|-----------|---------------------|----------------------|--------------------|\n",
    "| **Context Recall**               | 0.8554    | 0.5478              | 0.7794               | 0.5978             |\n",
    "| **Faithfulness**                 | 0.9335    | 0.7615              | 0.8369               | 0.5556             |\n",
    "| **Factual Correctness (F1)**     | 0.6183    | 0.6700              | 0.7017               | 0.4950             |\n",
    "| **Answer Relevancy**            | 0.7784    | 0.8553              | 0.9258               | 0.6250             |\n",
    "| **Context Entity Recall**        | 0.5048    | 0.4344              | 0.4364               | 0.5267             |\n",
    "| **Noise Sensitivity (Relevant)** | 0.1974    | 0.1552              | 0.1533               | 0.0492             |\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- **Naive Chunking** performs well in **faithfulness** and **context recall** due to broad document coverage but introduces the most **noise**.\n",
    "- **Semantic (k=5)** improves **factual correctness** and **answer relevancy**, but sacrifices **recall** by returning too few documents.\n",
    "- **Semantic (k=20)** is the strongest overall: it offers high **recall**, **faithfulness**, and **relevancy**, while keeping noise moderate.\n",
    "- **Cohere Rerank** surprisingly underperforms in **faithfulness** and **factual correctness**, despite excelling in **noise sensitivity** and **context entity recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## â— Why Did Cohere Underperform?\n",
    "\n",
    "The reranker pipeline used:\n",
    "- `k = 25` documents from the vectorstore (via similarity search)\n",
    "- `top_n = 8` after reranking with Cohere (`rerank-v3.5`)\n",
    "\n",
    "While this configuration **reduced irrelevant content**, it also **over-pruned the retrieved set**, potentially excluding essential supporting context. As a result:\n",
    "- The model couldn't always **justify its answers** using the provided context â†’ lower **faithfulness**\n",
    "- It sometimes **missed key facts** â†’ lower **factual correctness**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Recommendation\n",
    "\n",
    "To improve performance while retaining the benefits of reranking:\n",
    "\n",
    "- Increase retrieval depth: `k = 50`\n",
    "- Expand rerank cutoff: `top_n = 15`\n",
    "\n",
    "This should restore more relevant information while keeping the quality boost from reranking.\n",
    "\n",
    "```python\n",
    "initial_docs = vectorstore.similarity_search(query, k=50)\n",
    "response = co.rerank(query=query, documents=initial_docs, top_n=15)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
